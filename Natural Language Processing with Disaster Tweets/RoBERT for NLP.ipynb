{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing with Disaster Tweets","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Competition Description\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it’s not always clear whether a person’s words are actually announcing a disaster. For example: author may use the word “ABLAZE”, but he means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.\n\nIn this competition, it was necessary to create a machine learning model that predicts which tweets are dedicated to real disasters and which are not. To do this, there was access to a data set of 10,000 tweets that were classified manually.","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Data preparation and initial analysis","metadata":{"editable":false}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet\nimport nltk\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nimport torch\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:04.027246Z","iopub.execute_input":"2023-08-17T06:43:04.027735Z","iopub.status.idle":"2023-08-17T06:43:04.037564Z","shell.execute_reply.started":"2023-08-17T06:43:04.027695Z","shell.execute_reply":"2023-08-17T06:43:04.035437Z"},"editable":false,"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:04.193491Z","iopub.execute_input":"2023-08-17T06:43:04.194052Z","iopub.status.idle":"2023-08-17T06:43:04.240239Z","shell.execute_reply.started":"2023-08-17T06:43:04.194009Z","shell.execute_reply":"2023-08-17T06:43:04.237963Z"},"editable":false,"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:04.361769Z","iopub.execute_input":"2023-08-17T06:43:04.362272Z","iopub.status.idle":"2023-08-17T06:43:04.384545Z","shell.execute_reply.started":"2023-08-17T06:43:04.362235Z","shell.execute_reply":"2023-08-17T06:43:04.382737Z"},"editable":false,"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"tain_keywords = train.query('keyword.notna()')\ntain_keywords['keyword'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:04.564033Z","iopub.execute_input":"2023-08-17T06:43:04.564553Z","iopub.status.idle":"2023-08-17T06:43:04.580802Z","shell.execute_reply.started":"2023-08-17T06:43:04.564512Z","shell.execute_reply":"2023-08-17T06:43:04.579305Z"},"editable":false,"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array(['ablaze', 'accident', 'aftershock', 'airplane%20accident',\n       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n       'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked',\n       'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze',\n       'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood',\n       'bloody', 'blown%20up', 'body%20bag', 'body%20bagging',\n       'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse',\n       'buildings%20burning', 'buildings%20on%20fire', 'burned',\n       'burning', 'burning%20buildings', 'bush%20fires', 'casualties',\n       'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency',\n       'cliff%20fall', 'collapse', 'collapsed', 'collide', 'collided',\n       'collision', 'crash', 'crashed', 'crush', 'crushed', 'curfew',\n       'cyclone', 'damage', 'danger', 'dead', 'death', 'deaths', 'debris',\n       'deluge', 'deluged', 'demolish', 'demolished', 'demolition',\n       'derail', 'derailed', 'derailment', 'desolate', 'desolation',\n       'destroy', 'destroyed', 'destruction', 'detonate', 'detonation',\n       'devastated', 'devastation', 'disaster', 'displaced', 'drought',\n       'drown', 'drowned', 'drowning', 'dust%20storm', 'earthquake',\n       'electrocute', 'electrocuted', 'emergency', 'emergency%20plan',\n       'emergency%20services', 'engulfed', 'epicentre', 'evacuate',\n       'evacuated', 'evacuation', 'explode', 'exploded', 'explosion',\n       'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear',\n       'fire', 'fire%20truck', 'first%20responders', 'flames',\n       'flattened', 'flood', 'flooding', 'floods', 'forest%20fire',\n       'forest%20fires', 'hail', 'hailstorm', 'harm', 'hazard',\n       'hazardous', 'heat%20wave', 'hellfire', 'hijack', 'hijacker',\n       'hijacking', 'hostage', 'hostages', 'hurricane', 'injured',\n       'injuries', 'injury', 'inundated', 'inundation', 'landslide',\n       'lava', 'lightning', 'loud%20bang', 'mass%20murder',\n       'mass%20murderer', 'massacre', 'mayhem', 'meltdown', 'military',\n       'mudslide', 'natural%20disaster', 'nuclear%20disaster',\n       'nuclear%20reactor', 'obliterate', 'obliterated', 'obliteration',\n       'oil%20spill', 'outbreak', 'pandemonium', 'panic', 'panicking',\n       'police', 'quarantine', 'quarantined', 'radiation%20emergency',\n       'rainstorm', 'razed', 'refugees', 'rescue', 'rescued', 'rescuers',\n       'riot', 'rioting', 'rubble', 'ruin', 'sandstorm', 'screamed',\n       'screaming', 'screams', 'seismic', 'sinkhole', 'sinking', 'siren',\n       'sirens', 'smoke', 'snowstorm', 'storm', 'stretcher',\n       'structural%20failure', 'suicide%20bomb', 'suicide%20bomber',\n       'suicide%20bombing', 'sunk', 'survive', 'survived', 'survivors',\n       'terrorism', 'terrorist', 'threat', 'thunder', 'thunderstorm',\n       'tornado', 'tragedy', 'trapped', 'trauma', 'traumatised',\n       'trouble', 'tsunami', 'twister', 'typhoon', 'upheaval',\n       'violent%20storm', 'volcano', 'war%20zone', 'weapon', 'weapons',\n       'whirlwind', 'wild%20fires', 'wildfire', 'windstorm', 'wounded',\n       'wounds', 'wreck', 'wreckage', 'wrecked'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"tain_keywords.query('keyword==\"tsunami\"')","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:04.711154Z","iopub.execute_input":"2023-08-17T06:43:04.712718Z","iopub.status.idle":"2023-08-17T06:43:04.735090Z","shell.execute_reply.started":"2023-08-17T06:43:04.712668Z","shell.execute_reply":"2023-08-17T06:43:04.732838Z"},"editable":false,"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"         id  keyword                location  \\\n6943   9958  tsunami                     NaN   \n6944   9960  tsunami                     NaN   \n6945   9961  tsunami      in the Word of God   \n6946   9963  tsunami      in the Word of God   \n6947   9965  tsunami          Washington, DC   \n6948   9967  tsunami                     NaN   \n6949   9971  tsunami             Louavul, KY   \n6950   9972  tsunami                     NaN   \n6951   9973  tsunami                     NaN   \n6952   9974  tsunami  IG : Sincerely_TSUNAMI   \n6953   9976  tsunami         Kleenex factory   \n6954   9978  tsunami          ??????????????   \n6955   9979  tsunami                     NaN   \n6956   9980  tsunami                     NaN   \n6957   9982  tsunami       Land Of The Kings   \n6958   9983  tsunami   Winter Park, Colorado   \n6959   9984  tsunami                     NaN   \n6960   9985  tsunami         The Netherlands   \n6961   9986  tsunami                    #ODU   \n6962   9987  tsunami  ona block w/ my BOY ??   \n6963   9988  tsunami          East Islip, NY   \n6964   9989  tsunami    but i love kaylen ??   \n6965   9990  tsunami             COMING SOON   \n6966   9991  tsunami      in the Word of God   \n6967   9992  tsunami                     NaN   \n6968   9994  tsunami              Austin, TX   \n6969   9995  tsunami         Gotham City,USA   \n6970   9998  tsunami                     NaN   \n6971  10000  tsunami                     NaN   \n6972  10001  tsunami                  Hawaii   \n6973  10003  tsunami    BROKE NIGGAS DREAM!!   \n6974  10004  tsunami      in the Word of God   \n6975  10005  tsunami                     NaN   \n6976  10006  tsunami                     NaN   \n\n                                                   text  target  \n6943                                 I feel so lucky rn       0  \n6944  So did we have a hurricane tornado tsunami? So...       1  \n6945  @helene_yancey GodsLove &amp; #thankU my siste...       1  \n6946  @freefromwolves GodsLove &amp; #thankU brother...       0  \n6947  I'm at Baan Thai / Tsunami Sushi in Washington...       0  \n6948                      she keep it wet like tsunami.       0  \n6949  #BBShelli seems pretty sure she's the one that...       0  \n6950  Crptotech tsunami and banks.\\n http://t.co/KHz...       1  \n6951  #sing #tsunami Beginners #computer tutorial.: ...       0  \n6952        It's my senior year I just wanna go all out       0  \n6953  An optical illusion - clouds rolling in over t...       1  \n6954  Earthquake and tsunami that occurred in Japan ...       1  \n6955  Tsunami - DVBBS &amp; Borgeous (Arceen Festiva...       0  \n6956  #sing #tsunami Beginners #computer tutorial.: ...       0  \n6957                            @tsunami_esh ?? hey Esh       0  \n6958  'Anyway' the old lady went on 'I have somethin...       0  \n6959  6 Trends Are Driving a Data Tsunami for Startu...       0  \n6960  ?#FUKUSHIMA?#TEPCO?\\nMountains of debris from ...       1  \n6961                @TSUNAMI_nopeach ?????? I'm weak af       0  \n6962             Man my stomach feel like a tsunami ??.       0  \n6963  @slone did the First World War Ever Truly End?...       1  \n6964  I hope this tsunami clears b4 i have to walk o...       1  \n6965                      @tsunami_esh ESH PLEASE OKAY!       0  \n6966  @author_mike Amen today is the Day of Salvatio...       1  \n6967  and i dont get waves of missing you anymore th...       1  \n6968  Dr. Jim &amp; the tsunami: The latest New York...       0  \n6969  I don't get waves of missing you anymore... Th...       0  \n6970  @Kamunt Holy crap it's been forever since I sa...       0  \n6971  I liked a @YouTube video http://t.co/0h7OUa1pn...       0  \n6972  Meet Brinco your own personal earthquake snd t...       1  \n6973                       I want some tsunami take out       0  \n6974  @GreenLacey GodsLove &amp; #thankU my sister f...       0  \n6975                                 All of this energy       0  \n6976                 @Eric_Tsunami worry about yourself       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6943</th>\n      <td>9958</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>I feel so lucky rn</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6944</th>\n      <td>9960</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>So did we have a hurricane tornado tsunami? So...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6945</th>\n      <td>9961</td>\n      <td>tsunami</td>\n      <td>in the Word of God</td>\n      <td>@helene_yancey GodsLove &amp;amp; #thankU my siste...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6946</th>\n      <td>9963</td>\n      <td>tsunami</td>\n      <td>in the Word of God</td>\n      <td>@freefromwolves GodsLove &amp;amp; #thankU brother...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6947</th>\n      <td>9965</td>\n      <td>tsunami</td>\n      <td>Washington, DC</td>\n      <td>I'm at Baan Thai / Tsunami Sushi in Washington...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6948</th>\n      <td>9967</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>she keep it wet like tsunami.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6949</th>\n      <td>9971</td>\n      <td>tsunami</td>\n      <td>Louavul, KY</td>\n      <td>#BBShelli seems pretty sure she's the one that...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6950</th>\n      <td>9972</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>Crptotech tsunami and banks.\\n http://t.co/KHz...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6951</th>\n      <td>9973</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>#sing #tsunami Beginners #computer tutorial.: ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6952</th>\n      <td>9974</td>\n      <td>tsunami</td>\n      <td>IG : Sincerely_TSUNAMI</td>\n      <td>It's my senior year I just wanna go all out</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6953</th>\n      <td>9976</td>\n      <td>tsunami</td>\n      <td>Kleenex factory</td>\n      <td>An optical illusion - clouds rolling in over t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6954</th>\n      <td>9978</td>\n      <td>tsunami</td>\n      <td>??????????????</td>\n      <td>Earthquake and tsunami that occurred in Japan ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6955</th>\n      <td>9979</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>Tsunami - DVBBS &amp;amp; Borgeous (Arceen Festiva...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6956</th>\n      <td>9980</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>#sing #tsunami Beginners #computer tutorial.: ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6957</th>\n      <td>9982</td>\n      <td>tsunami</td>\n      <td>Land Of The Kings</td>\n      <td>@tsunami_esh ?? hey Esh</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6958</th>\n      <td>9983</td>\n      <td>tsunami</td>\n      <td>Winter Park, Colorado</td>\n      <td>'Anyway' the old lady went on 'I have somethin...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6959</th>\n      <td>9984</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>6 Trends Are Driving a Data Tsunami for Startu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6960</th>\n      <td>9985</td>\n      <td>tsunami</td>\n      <td>The Netherlands</td>\n      <td>?#FUKUSHIMA?#TEPCO?\\nMountains of debris from ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6961</th>\n      <td>9986</td>\n      <td>tsunami</td>\n      <td>#ODU</td>\n      <td>@TSUNAMI_nopeach ?????? I'm weak af</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6962</th>\n      <td>9987</td>\n      <td>tsunami</td>\n      <td>ona block w/ my BOY ??</td>\n      <td>Man my stomach feel like a tsunami ??.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6963</th>\n      <td>9988</td>\n      <td>tsunami</td>\n      <td>East Islip, NY</td>\n      <td>@slone did the First World War Ever Truly End?...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6964</th>\n      <td>9989</td>\n      <td>tsunami</td>\n      <td>but i love kaylen ??</td>\n      <td>I hope this tsunami clears b4 i have to walk o...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6965</th>\n      <td>9990</td>\n      <td>tsunami</td>\n      <td>COMING SOON</td>\n      <td>@tsunami_esh ESH PLEASE OKAY!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6966</th>\n      <td>9991</td>\n      <td>tsunami</td>\n      <td>in the Word of God</td>\n      <td>@author_mike Amen today is the Day of Salvatio...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6967</th>\n      <td>9992</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>and i dont get waves of missing you anymore th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6968</th>\n      <td>9994</td>\n      <td>tsunami</td>\n      <td>Austin, TX</td>\n      <td>Dr. Jim &amp;amp; the tsunami: The latest New York...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6969</th>\n      <td>9995</td>\n      <td>tsunami</td>\n      <td>Gotham City,USA</td>\n      <td>I don't get waves of missing you anymore... Th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6970</th>\n      <td>9998</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>@Kamunt Holy crap it's been forever since I sa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6971</th>\n      <td>10000</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>I liked a @YouTube video http://t.co/0h7OUa1pn...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6972</th>\n      <td>10001</td>\n      <td>tsunami</td>\n      <td>Hawaii</td>\n      <td>Meet Brinco your own personal earthquake snd t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6973</th>\n      <td>10003</td>\n      <td>tsunami</td>\n      <td>BROKE NIGGAS DREAM!!</td>\n      <td>I want some tsunami take out</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6974</th>\n      <td>10004</td>\n      <td>tsunami</td>\n      <td>in the Word of God</td>\n      <td>@GreenLacey GodsLove &amp;amp; #thankU my sister f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6975</th>\n      <td>10005</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>All of this energy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6976</th>\n      <td>10006</td>\n      <td>tsunami</td>\n      <td>NaN</td>\n      <td>@Eric_Tsunami worry about yourself</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Most of the omissions in the keywords and location attributes cannot be replaced with specific values for several reasons:\n\n1. The data is filled in manually. Most likely, the omissions are not accidental\n2. In some answers, it is not possible to clearly define the location and keyword","metadata":{"editable":false}},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:05.058729Z","iopub.execute_input":"2023-08-17T06:43:05.059251Z","iopub.status.idle":"2023-08-17T06:43:05.067915Z","shell.execute_reply.started":"2023-08-17T06:43:05.059199Z","shell.execute_reply":"2023-08-17T06:43:05.066950Z"},"editable":false,"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0    4342\n1    3271\nName: target, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"The balance of classes is almost met","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Preparation and training of the RoBERT model","metadata":{"editable":false}},{"cell_type":"code","source":"# Load pre-trained RoBERTa tokenizer and model\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:06.405591Z","iopub.execute_input":"2023-08-17T06:43:06.406071Z","iopub.status.idle":"2023-08-17T06:43:07.916999Z","shell.execute_reply.started":"2023-08-17T06:43:06.406027Z","shell.execute_reply":"2023-08-17T06:43:07.915663Z"},"editable":false,"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"nltk.data.path.append('/kaggle/working/nltk_data/')\n\nnltk.download('wordnet', download_dir='/kaggle/working/nltk_data/')\nnltk.download('averaged_perceptron_tagger', download_dir='/kaggle/working/nltk_data/')\nnltk.download('stopwords', download_dir='/kaggle/working/nltk_data/')\nnltk.download('omw-1.4', download_dir='/kaggle/working/nltk_data/')\nnltk.download('punkt', download_dir='/kaggle/working/nltk_data/')\nnltk.download('wordnet2022')\n\n\n! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:07.918833Z","iopub.execute_input":"2023-08-17T06:43:07.919192Z","iopub.status.idle":"2023-08-17T06:43:08.415024Z","shell.execute_reply.started":"2023-08-17T06:43:07.919165Z","shell.execute_reply":"2023-08-17T06:43:08.413373Z"},"editable":false,"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package punkt to /kaggle/working/nltk_data/...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet2022 to /usr/share/nltk_data...\n[nltk_data]   Package wordnet2022 is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Functions for lemmatization and text cleaning\n\ndef lemmatize(text):\n    m = WordNetLemmatizer()\n    word_list = word_tokenize(text)\n    tagged_words = nltk.pos_tag(word_list)\n    lemmatized_words = []\n\n    for word, tag in tagged_words:\n        if tag.startswith('NN'):\n            lemmatized_words.append(m.lemmatize(word, pos='n'))\n        elif tag.startswith('VB'):\n            lemmatized_words.append(m.lemmatize(word, pos='v'))\n        elif tag.startswith('JJ'):\n            lemmatized_words.append(m.lemmatize(word, pos='a'))\n        elif tag.startswith('R'):\n            lemmatized_words.append(m.lemmatize(word, pos='r'))\n        else:\n            lemmatized_words.append(m.lemmatize(word))\n\n    return ' '.join(lemmatized_words)\n\ndef clear_text(text):\n    text = re.sub(r\"[^a-zA-Z']\", ' ', text)\n    return \" \".join(text.split())","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:08.417531Z","iopub.execute_input":"2023-08-17T06:43:08.418482Z","iopub.status.idle":"2023-08-17T06:43:08.429231Z","shell.execute_reply.started":"2023-08-17T06:43:08.418400Z","shell.execute_reply":"2023-08-17T06:43:08.427156Z"},"editable":false,"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].apply(clear_text)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:08.969185Z","iopub.execute_input":"2023-08-17T06:43:08.969554Z","iopub.status.idle":"2023-08-17T06:43:09.023711Z","shell.execute_reply.started":"2023-08-17T06:43:08.969526Z","shell.execute_reply":"2023-08-17T06:43:09.022435Z"},"editable":false,"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train['lemmatize_text'] = train['text'].apply(lemmatize)\ntrain = train.drop(['text'], axis=1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:09.678436Z","iopub.execute_input":"2023-08-17T06:43:09.678801Z","iopub.status.idle":"2023-08-17T06:43:19.862334Z","shell.execute_reply.started":"2023-08-17T06:43:09.678772Z","shell.execute_reply":"2023-08-17T06:43:19.860928Z"},"editable":false,"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"   id keyword location  target  \\\n0   1     NaN      NaN       1   \n1   4     NaN      NaN       1   \n2   5     NaN      NaN       1   \n3   6     NaN      NaN       1   \n4   7     NaN      NaN       1   \n\n                                      lemmatize_text  \n0  Our Deeds be the Reason of this earthquake May...  \n1              Forest fire near La Ronge Sask Canada  \n2  All resident ask to 'shelter in place ' be be ...  \n3  people receive wildfire evacuation order in Ca...  \n4  Just get send this photo from Ruby Alaska a sm...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>target</th>\n      <th>lemmatize_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>Our Deeds be the Reason of this earthquake May...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>Forest fire near La Ronge Sask Canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>All resident ask to 'shelter in place ' be be ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>people receive wildfire evacuation order in Ca...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>Just get send this photo from Ruby Alaska a sm...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"corpus = train['lemmatize_text']\ntest_corpus = test['text']\n\ntarget_train = train['target']\n\nfeatures_train = []\nfeatures_test = []","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:19.865018Z","iopub.execute_input":"2023-08-17T06:43:19.865378Z","iopub.status.idle":"2023-08-17T06:43:19.871576Z","shell.execute_reply.started":"2023-08-17T06:43:19.865350Z","shell.execute_reply":"2023-08-17T06:43:19.870476Z"},"editable":false,"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Convert text to RoBERTa embeddings\nfor text in corpus:\n    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        embeddings = model(**tokens).last_hidden_state.mean(dim=1)\n        features_train.append(embeddings)\n\nfeatures_train = torch.cat(features_train, dim=0)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:43:19.873683Z","iopub.execute_input":"2023-08-17T06:43:19.874486Z","iopub.status.idle":"2023-08-17T06:54:41.806728Z","shell.execute_reply.started":"2023-08-17T06:43:19.874443Z","shell.execute_reply":"2023-08-17T06:54:41.805508Z"},"editable":false,"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:54:41.809227Z","iopub.execute_input":"2023-08-17T06:54:41.810627Z","iopub.status.idle":"2023-08-17T06:54:41.815251Z","shell.execute_reply.started":"2023-08-17T06:54:41.810589Z","shell.execute_reply":"2023-08-17T06:54:41.813921Z"},"editable":false,"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"logreg_model = LogisticRegression(max_iter=200, C=5, class_weight='balanced')\nlogreg_scores = []\n\nfor train_index, val_index in kf.split(features_train):\n    train_features, val_features = features_train[train_index], features_train[val_index]\n    train_target, val_target = target_train[train_index], target_train[val_index]\n\n    logreg_model.fit(train_features, train_target)\n    val_predictions = logreg_model.predict(val_features)\n    val_f1 = f1_score(val_target, val_predictions)\n    logreg_scores.append(val_f1)\n\nlogreg_f1 = sum(logreg_scores) / len(logreg_scores)\nprint('Logistic regression, F1-measure on cross-validation:', logreg_f1)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T06:54:41.819095Z","iopub.execute_input":"2023-08-17T06:54:41.819596Z","iopub.status.idle":"2023-08-17T06:54:46.248458Z","shell.execute_reply.started":"2023-08-17T06:54:41.819562Z","shell.execute_reply":"2023-08-17T06:54:46.247712Z"},"editable":false,"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic regression, F1-measure on cross-validation: 0.774748675773278\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Predicting the results of a target feature on a test sample","metadata":{"editable":false}},{"cell_type":"code","source":"for text in test_corpus:\n    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        embeddings = model(**tokens).last_hidden_state.mean(dim=1)\n        features_test.append(embeddings)\n\nfeatures_test = torch.cat(features_test, dim=0)\ntest_predictions = logreg_model.predict(features_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T12:57:52.674975Z","iopub.execute_input":"2023-08-15T12:57:52.675682Z","iopub.status.idle":"2023-08-15T13:03:09.347277Z","shell.execute_reply.started":"2023-08-15T12:57:52.675638Z","shell.execute_reply":"2023-08-15T13:03:09.345547Z"},"editable":false,"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame with the predicted results\nsubmission = pd.DataFrame({'id': test['id'], 'target': test_predictions})\n\n# Save the predictions to a CSV file\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T13:03:09.349621Z","iopub.execute_input":"2023-08-15T13:03:09.350646Z","iopub.status.idle":"2023-08-15T13:03:09.383638Z","shell.execute_reply.started":"2023-08-15T13:03:09.350585Z","shell.execute_reply":"2023-08-15T13:03:09.382009Z"},"editable":false,"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2023-08-15T13:03:09.386470Z","iopub.execute_input":"2023-08-15T13:03:09.387800Z","iopub.status.idle":"2023-08-15T13:03:09.413416Z","shell.execute_reply.started":"2023-08-15T13:03:09.387730Z","shell.execute_reply":"2023-08-15T13:03:09.411777Z"},"editable":false,"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"         id  target\n0         0       1\n1         2       1\n2         3       1\n3         9       1\n4        11       1\n...     ...     ...\n3258  10861       1\n3259  10865       1\n3260  10868       1\n3261  10874       1\n3262  10875       0\n\n[3263 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3258</th>\n      <td>10861</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3259</th>\n      <td>10865</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3260</th>\n      <td>10868</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>10875</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 2 columns</p>\n</div>"},"metadata":{}}]}]}